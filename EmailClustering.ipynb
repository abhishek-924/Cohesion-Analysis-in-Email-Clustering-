{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.preprocessing import normalize \n",
    "import nltk.stem.porter\n",
    "porter_stemmer=nltk.stem.porter.PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will extract raw message from the message object\n",
    "def raw_message(raw_message):\n",
    "    lines = raw_message.split('\\n') #Here, the paragraph contains \"\\n\" indicating the new line. Hence, the sentence tokenizer is not used.\n",
    "    email = {}\n",
    "    message = ''\n",
    "    keys_to_extract = ['from', 'to']\n",
    "    for line in lines:\n",
    "        if ':' not in line:\n",
    "            message += line.strip() #removes all the spaces and appends only the text body to the variable \"message\"\n",
    "            email['body'] = message\n",
    "        else:\n",
    "            pairs = line.split(':') #This enables to divide each sentence into 2 parts\n",
    "            key = pairs[0].lower()\n",
    "            val = pairs[1].strip()\n",
    "            if key in keys_to_extract:\n",
    "                email[key] = val\n",
    "    return email  #returns an array with key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_into_emails(messages):\n",
    "    emails = [raw_message(message) for message in messages]\n",
    "    return {\n",
    "        'body': map_to_list(emails, 'body'), \n",
    "        'to': map_to_list(emails, 'to'), \n",
    "        'from_': map_to_list(emails, 'from')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_list(emails, key):\n",
    "    results = []\n",
    "    for email in emails:\n",
    "        if key not in email:\n",
    "            results.append('')\n",
    "        else:\n",
    "            results.append(email[key])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_in_doc(X, features, row_id, top_n=25):\n",
    "    row = np.squeeze(X[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=20):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "emails = pd.read_csv('split_emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "email_df = pd.DataFrame(parse_into_emails(emails.message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop emails with empty body, to or from_ columns.\n",
    "email_df.drop(email_df.query(\"body == '' | to == '' | from_ == ''\").index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding top words\n",
    "stopwords = ENGLISH_STOP_WORDS.union(['ect', 'hou', 'com', 'recipient'])\n",
    "vect = TfidfVectorizer(analyzer='word', stop_words=stopwords, max_df=0.3, min_df=2)\n",
    "# max_df=0.5 means \"ignore all terms that appear in more then 50% of the documents\"\n",
    "# min_df=2 means \"ignore all terms that appear in less then 2 documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "body = email_df.body\n",
    "def stemmingDF(body):\n",
    "    bigl=[]\n",
    "    for string in body:\n",
    "        lis=string.split()\n",
    "        l=[]\n",
    "        for word in lis:\n",
    "            if word.isalpha()==True:                \n",
    "                if word not in stopwords:\n",
    "                    if len(word)!=1:\n",
    "                        word=word.lower()\n",
    "                        word=lemmatizer.lemmatize(word)\n",
    "                        l.append(word)\n",
    "        str1=' '.join(l)\n",
    "        bigl.append(str1)\n",
    "    return bigl\n",
    "\n",
    "email_df[\"body_new\"] = stemmingDF(body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#email_df.body_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print top features\n",
    "X = vect.fit_transform(email_df.body_new)\n",
    "features = vect.get_feature_names()\n",
    "#print(top_feats_in_doc(X, features, 1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "wcss = []\n",
    "for i in range(1, 6):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X.todense())\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 6), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "clf = KMeans(n_clusters=n_clusters, \n",
    "            max_iter=100, \n",
    "            init='k-means++', \n",
    "            n_init=1)\n",
    "labels = clf.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"cluster = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.score, align='center', color='#7530FF')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.features)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_per_cluster(X, y, features, min_tfidf=0.1, top_n=25):\n",
    "    dfs = []\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label) \n",
    "        feats_df = top_mean_feats(X, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(X, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    if grp_ids:\n",
    "        D = X[grp_ids].toarray()\n",
    "    else:\n",
    "        D = X.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense = X.todense()\n",
    "pca = PCA(n_components=2).fit(X_dense)\n",
    "coords = pca.transform(X_dense)\n",
    "\n",
    "# Lets plot it again, but this time we add some color to it.\n",
    "# This array needs to be at least the length of the n_clusters.\n",
    "label_colors = [\"#2AB0E9\", \"#2BAF74\", \"#D7665E\", \"#CCCCCC\", \n",
    "                \"#D2CA0D\", \"#522A64\", \"#A3DB05\", \"#FC6514\"]\n",
    "colors = [label_colors[i] for i in labels]\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], c=colors)\n",
    "# Plot the cluster centers\n",
    "centroids = clf.cluster_centers_\n",
    "centroid_coords = pca.transform(centroids)\n",
    "plt.scatter(centroid_coords[:, 0], centroid_coords[:, 1], marker='X', s=200, linewidths=2, c='#444d60')\n",
    "plt.show()\n",
    "\n",
    "#Use this to print the top terms per cluster with matplotlib.\n",
    "plot_tfidf_classfeats_h(top_feats_per_cluster(X, labels, features, 0.1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_score(lst):   \n",
    "    leve_m=np.zeros([10,10])\n",
    "    for i,word1 in enumerate(lst['features']):\n",
    "        for j,word2 in enumerate(lst['features']):\n",
    "            if word1!=word2:\n",
    "                leve_m[i][j]=linear_kernel(lst['score'][i].reshape(-1,1),lst['score'][j].reshape(-1,1)).flatten()\n",
    "    return leve_m.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst0=top_feats_per_cluster(X,labels,features,0.1,10)[0]\n",
    "lst1=top_feats_per_cluster(X,labels,features,0.1,10)[1]\n",
    "lst2=top_feats_per_cluster(X,labels,features,0.1,10)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score0=cos_score(lst0)\n",
    "score1=cos_score(lst1)\n",
    "score2=cos_score(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score0)\n",
    "print(score1)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=X.todense()\n",
    "from scipy.cluster import hierarchy\n",
    "Z=hierarchy.linkage(X_new, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = hierarchy.dendrogram(Z)\n",
    "#dendrogram=sch.dendrogram(sch,linkage(X,method='ward'))\n",
    "plt.title('Dendrogram')\n",
    "#plt.xlabel('Customers')\n",
    "plt.ylabel('Euclidean distances')\n",
    "\n",
    "plt.xlim(0,5000)\n",
    "plt.ylim(0,15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = hc.fit_predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hc1=y_hc.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "def cos_score(lst):   \n",
    "    leve_m=np.zeros([10,10])\n",
    "    for i,word1 in enumerate(lst['features']):\n",
    "        for j,word2 in enumerate(lst['features']):\n",
    "            if word1!=word2:\n",
    "                leve_m[i][j]=linear_kernel(lst['score'][i].reshape(-1,1),lst['score'][j].reshape(-1,1)).flatten()\n",
    "    return leve_m.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst0=top_feats_per_cluster(X,y_hc1,features,0.1,10)[0]\n",
    "lst1=top_feats_per_cluster(X,y_hc1,features,0.1,10)[1]\n",
    "lst2=top_feats_per_cluster(X,y_hc1,features,0.1,10)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score0=cos_score(lst0)\n",
    "\n",
    "score1=cos_score(lst1)\n",
    "score2=cos_score(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score0)\n",
    "print(score1)\n",
    "print(score2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
